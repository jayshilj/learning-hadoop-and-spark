# Learning Hadoop and Spark

### Contents

This is the companion repo to my LinkedIn Learning Courses on Hadoop and Spark.  
1. **Learning Hadoop** - [link](https://www.linkedin.com/learning/learning-hadoop-2) uses mostly GCP Dataproc for running Hadoop and associated libraries (i.e. Hive, Pig, Spark...) workloads
2. **Cloud Hadoop: Scaling Apache Spark** - [link](https://www.linkedin.com/learning/cloud-hadoop-scaling-apache-spark) - uses GCP DataProc, AWS EMR or Databricks on AWS
3. **Azure Databricks Spark Essential Training** - [link](https://www.linkedin.com/learning/azure-databricks-essential-training) uses Azure with Databricks for scaling Apache Spark workloads
